// Enhanced ML models with better performance and features
use std::collections::HashMap;
use std::f64::consts::PI;
use crate::types::Candle;

// ============================================================================
// ADVANCED FEATURE ENGINEERING
// ============================================================================

#[derive(Debug, Clone)]
pub struct EnhancedFeatureSet {
    pub features: Vec<f64>,
    pub labels: Vec<String>,
    pub target: Option<f64>,
    pub timestamp: u64,
    pub metadata: FeatureMetadata,
}

#[derive(Debug, Clone)]
pub struct FeatureMetadata {
    pub volatility_regime: VolatilityRegime,
    pub trend_regime: TrendRegime,
    pub market_phase: MarketPhase,
}

#[derive(Debug, Clone)]
pub enum VolatilityRegime {
    Low,
    Medium,
    High,
    Extreme,
}

#[derive(Debug, Clone)]
pub enum TrendRegime {
    StrongUptrend,
    WeakUptrend,
    Sideways,
    WeakDowntrend,
    StrongDowntrend,
}

#[derive(Debug, Clone)]
pub enum MarketPhase {
    Accumulation,
    MarkupTrend,
    Distribution,
    MarkdownTrend,
}

pub struct AdvancedFeatureEngineer;

impl AdvancedFeatureEngineer {
    /// Extract comprehensive features with market regime awareness
    pub fn extract_enhanced_features(
        candles: &[Candle],
        indicator_map: &HashMap<String, Vec<Option<f64>>>,
        lookback_window: usize,
    ) -> Vec<EnhancedFeatureSet> {
        let mut feature_sets = Vec::new();
        
        for i in lookback_window..candles.len() {
            let mut features = Vec::new();
            let mut labels = Vec::new();
            
            // 1. Price Action Features (Enhanced)
            let price_features = Self::extract_price_action_features(&candles, i, lookback_window);
            features.extend(price_features.0);
            labels.extend(price_features.1);
            
            // 2. Volume Profile Features
            let volume_features = Self::extract_volume_profile_features(&candles, i, lookback_window);
            features.extend(volume_features.0);
            labels.extend(volume_features.1);
            
            // 3. Volatility Features
            let volatility_features = Self::extract_volatility_features(&candles, i, lookback_window);
            features.extend(volatility_features.0);
            labels.extend(volatility_features.1);
            
            // 4. Market Microstructure Features
            let microstructure_features = Self::extract_microstructure_features(&candles, i, lookback_window);
            features.extend(microstructure_features.0);
            labels.extend(microstructure_features.1);
            
            // 5. Technical Indicator Features (Enhanced)
            let tech_features = Self::extract_enhanced_technical_features(indicator_map, i);
            features.extend(tech_features.0);
            labels.extend(tech_features.1);
            
            // 6. Time-based Features
            let time_features = Self::extract_time_features(&candles[i]);
            features.extend(time_features.0);
            labels.extend(time_features.1);
            
            // 7. Cross-sectional Features
            let cross_features = Self::extract_cross_sectional_features(&candles, i, lookback_window);
            features.extend(cross_features.0);
            labels.extend(cross_features.1);
            
            // Market Regime Classification
            let metadata = Self::classify_market_regime(&candles, i, lookback_window);
            
            // Multiple prediction targets
            let target = if i + 1 < candles.len() {
                Some((candles[i+1].close - candles[i].close) / candles[i].close)
            } else {
                None
            };
            
            feature_sets.push(EnhancedFeatureSet {
                features,
                labels,
                target,
                timestamp: candles[i].timestamp,
                metadata,
            });
        }
        
        feature_sets
    }
    
    fn extract_price_action_features(
        candles: &[Candle], 
        idx: usize, 
        window: usize
    ) -> (Vec<f64>, Vec<String>) {
        let mut features = Vec::new();
        let mut labels = Vec::new();
        
        let start = idx.saturating_sub(window);
        let window_candles = &candles[start..=idx];
        
        // Multi-timeframe returns
        for period in [1, 2, 3, 5, 10, 20] {
            if idx >= period {
                let ret = (candles[idx].close - candles[idx - period].close) / candles[idx - period].close;
                features.push(ret);
                labels.push(format!("return_{}d", period));
            }
        }
        
        // Price position within recent range
        let highs: Vec<f64> = window_candles.iter().map(|c| c.high).collect();
        let lows: Vec<f64> = window_candles.iter().map(|c| c.low).collect();
        
        let max_high = highs.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
        let min_low = lows.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        
        if max_high != min_low {
            let price_position = (candles[idx].close - min_low) / (max_high - min_low);
            features.push(price_position);
            labels.push("price_position".to_string());
        }
        
        // Gap analysis
        if idx > 0 {
            let gap = (candles[idx].open - candles[idx-1].close) / candles[idx-1].close;
            features.push(gap);
            labels.push("gap".to_string());
        }
        
        // Intraday momentum
        let intraday_return = (candles[idx].close - candles[idx].open) / candles[idx].open;
        features.push(intraday_return);
        labels.push("intraday_return".to_string());
        
        // Body-to-range ratio
        let body = (candles[idx].close - candles[idx].open).abs();
        let range = candles[idx].high - candles[idx].low;
        if range > 0.0 {
            features.push(body / range);
            labels.push("body_to_range".to_string());
        }
        
        (features, labels)
    }
    
    fn extract_volume_profile_features(
        candles: &[Candle], 
        idx: usize, 
        window: usize
    ) -> (Vec<f64>, Vec<String>) {
        let mut features = Vec::new();
        let mut labels = Vec::new();
        
        let start = idx.saturating_sub(window);
        let window_candles = &candles[start..=idx];
        
        // Volume ratios
        let current_volume = candles[idx].volume.unwrap_or(0.0);
        let avg_volume = window_candles.iter()
            .map(|c| c.volume.unwrap_or(0.0))
            .sum::<f64>() / window_candles.len() as f64;
        
        if avg_volume > 0.0 {
            features.push(current_volume / avg_volume);
            labels.push("volume_ratio".to_string());
        }
        
        // Price-Volume Correlation
        let prices: Vec<f64> = window_candles.iter().map(|c| c.close).collect();
        let volumes: Vec<f64> = window_candles.iter().map(|c| c.volume.unwrap_or(0.0)).collect();
        
        let pv_correlation = Self::calculate_correlation(&prices, &volumes);
        features.push(pv_correlation);
        labels.push("price_volume_correlation".to_string());
        
        // Volume-weighted metrics
        let vwap = Self::calculate_vwap(window_candles);
        if vwap > 0.0 {
            let vwap_deviation = (candles[idx].close - vwap) / vwap;
            features.push(vwap_deviation);
            labels.push("vwap_deviation".to_string());
        }
        
        (features, labels)
    }
    
    fn extract_volatility_features(
        candles: &[Candle], 
        idx: usize, 
        window: usize
    ) -> (Vec<f64>, Vec<String>) {
        let mut features = Vec::new();
        let mut labels = Vec::new();
        
        let start = idx.saturating_sub(window);
        let window_candles = &candles[start..=idx];
        
        // Realized volatility (multiple timeframes)
        for period in [5, 10, 20] {
            let vol = Self::calculate_realized_volatility(window_candles, period);
            features.push(vol);
            labels.push(format!("realized_vol_{}d", period));
        }
        
        // Volatility of volatility
        let vol_series: Vec<f64> = window_candles.windows(5)
            .map(|w| Self::calculate_realized_volatility(w, 5))
            .collect();
        
        if vol_series.len() > 1 {
            let vol_of_vol = Self::calculate_volatility(&vol_series);
            features.push(vol_of_vol);
            labels.push("volatility_of_volatility".to_string());
        }
        
        // Range-based volatility
        let range_vol = window_candles.iter()
            .map(|c| (c.high - c.low) / c.close)
            .sum::<f64>() / window_candles.len() as f64;
        features.push(range_vol);
        labels.push("range_volatility".to_string());
        
        (features, labels)
    }
    
    fn extract_microstructure_features(
        candles: &[Candle], 
        idx: usize, 
        window: usize
    ) -> (Vec<f64>, Vec<String>) {
        let mut features = Vec::new();
        let mut labels = Vec::new();
        
        let start = idx.saturating_sub(window);
        let window_candles = &candles[start..=idx];
        
        // Bid-Ask Spread Proxy (using high-low)
        let spread_proxy = (candles[idx].high - candles[idx].low) / candles[idx].close;
        features.push(spread_proxy);
        labels.push("spread_proxy".to_string());
        
        // Order Flow Imbalance Proxy
        let up_moves = window_candles.windows(2)
            .filter(|w| w[1].close > w[0].close)
            .count() as f64;
        let total_moves = window_candles.len().saturating_sub(1) as f64;
        
        if total_moves > 0.0 {
            let flow_imbalance = (up_moves / total_moves) - 0.5;
            features.push(flow_imbalance);
            labels.push("flow_imbalance".to_string());
        }
        
        // Tick Rule (simplified)
        if idx > 0 {
            let tick_direction = if candles[idx].close > candles[idx-1].close { 1.0 } 
                                else if candles[idx].close < candles[idx-1].close { -1.0 } 
                                else { 0.0 };
            features.push(tick_direction);
            labels.push("tick_direction".to_string());
        }
        
        (features, labels)
    }
    
    fn extract_enhanced_technical_features(
        indicator_map: &HashMap<String, Vec<Option<f64>>>,
        idx: usize,
    ) -> (Vec<f64>, Vec<String>) {
        let mut features = Vec::new();
        let mut labels = Vec::new();
        
        for (name, values) in indicator_map {
            if let Some(Some(value)) = values.get(idx) {
                features.push(*value);
                labels.push(name.clone());
                
                // Add rate of change for indicators
                if idx > 0 {
                    if let Some(Some(prev_value)) = values.get(idx - 1) {
                        let roc = (value - prev_value) / prev_value.abs().max(1e-8);
                        features.push(roc);
                        labels.push(format!("{}_roc", name));
                    }
                }
                
                // Add normalized values (z-score over recent period)
                let lookback = 20.min(idx);
                if lookback > 0 {
                    let recent_values: Vec<f64> = values.iter()
                        .skip(idx.saturating_sub(lookback))
                        .take(lookback)
                        .filter_map(|v| *v)
                        .collect();
                    
                    if recent_values.len() > 1 {
                        let mean = recent_values.iter().sum::<f64>() / recent_values.len() as f64;
                        let std = Self::calculate_volatility(&recent_values);
                        if std > 0.0 {
                            let z_score = (value - mean) / std;
                            features.push(z_score);
                            labels.push(format!("{}_zscore", name));
                        }
                    }
                }
            }
        }
        
        (features, labels)
    }
    
    fn extract_time_features(candle: &Candle) -> (Vec<f64>, Vec<String>) {
        let mut features = Vec::new();
        let mut labels = Vec::new();
        
        use chrono::{DateTime, Utc, Timelike, Datelike};
        
        let dt: DateTime<Utc> = DateTime::from_timestamp(candle.timestamp as i64, 0)
            .unwrap_or_else(|| Utc::now());
        
        // Cyclical time features
        let hour_sin = ((dt.hour() as f64 * 2.0 * PI) / 24.0).sin();
        let hour_cos = ((dt.hour() as f64 * 2.0 * PI) / 24.0).cos();
        features.extend_from_slice(&[hour_sin, hour_cos]);
        labels.extend_from_slice(&["hour_sin".to_string(), "hour_cos".to_string()]);
        
        let day_sin = ((dt.weekday().num_days_from_monday() as f64 * 2.0 * PI) / 7.0).sin();
        let day_cos = ((dt.weekday().num_days_from_monday() as f64 * 2.0 * PI) / 7.0).cos();
        features.extend_from_slice(&[day_sin, day_cos]);
        labels.extend_from_slice(&["day_sin".to_string(), "day_cos".to_string()]);
        
        let month_sin = ((dt.month() as f64 * 2.0 * PI) / 12.0).sin();
        let month_cos = ((dt.month() as f64 * 2.0 * PI) / 12.0).cos();
        features.extend_from_slice(&[month_sin, month_cos]);
        labels.extend_from_slice(&["month_sin".to_string(), "month_cos".to_string()]);
        
        (features, labels)
    }
    
    fn extract_cross_sectional_features(
        candles: &[Candle], 
        idx: usize, 
        window: usize
    ) -> (Vec<f64>, Vec<String>) {
        let mut features = Vec::new();
        let mut labels = Vec::new();
        
        let start = idx.saturating_sub(window);
        let window_candles = &candles[start..=idx];
        
        // Momentum persistence
        let returns: Vec<f64> = window_candles.windows(2)
            .map(|w| (w[1].close - w[0].close) / w[0].close)
            .collect();
        
        if returns.len() > 1 {
            let momentum_autocorr = Self::calculate_autocorrelation(&returns, 1);
            features.push(momentum_autocorr);
            labels.push("momentum_autocorr".to_string());
        }
        
        // Trend consistency
        let up_days = returns.iter().filter(|&&r| r > 0.0).count() as f64;
        let total_days = returns.len() as f64;
        if total_days > 0.0 {
            let trend_consistency = (up_days / total_days - 0.5).abs();
            features.push(trend_consistency);
            labels.push("trend_consistency".to_string());
        }
        
        (features, labels)
    }
    
    fn classify_market_regime(
        candles: &[Candle], 
        idx: usize, 
        window: usize
    ) -> FeatureMetadata {
        let start = idx.saturating_sub(window);
        let window_candles = &candles[start..=idx];
        
        // Volatility regime
        let vol = Self::calculate_realized_volatility(window_candles, window.min(20));
        let volatility_regime = match vol {
            v if v < 0.01 => VolatilityRegime::Low,
            v if v < 0.02 => VolatilityRegime::Medium,
            v if v < 0.04 => VolatilityRegime::High,
            _ => VolatilityRegime::Extreme,
        };
        
        // Trend regime
        let trend_strength = Self::calculate_trend_strength(window_candles);
        let trend_regime = match trend_strength {
            t if t > 0.02 => TrendRegime::StrongUptrend,
            t if t > 0.005 => TrendRegime::WeakUptrend,
            t if t < -0.02 => TrendRegime::StrongDowntrend,
            t if t < -0.005 => TrendRegime::WeakDowntrend,
            _ => TrendRegime::Sideways,
        };
        
        // Market phase (simplified)
        let market_phase = MarketPhase::MarkupTrend; // Placeholder
        
        FeatureMetadata {
            volatility_regime,
            trend_regime,
            market_phase,
        }
    }
    
    // Helper functions
    fn calculate_correlation(x: &[f64], y: &[f64]) -> f64 {
        if x.len() != y.len() || x.len() < 2 {
            return 0.0;
        }
        
        let n = x.len() as f64;
        let mean_x = x.iter().sum::<f64>() / n;
        let mean_y = y.iter().sum::<f64>() / n;
        
        let numerator: f64 = x.iter().zip(y)
            .map(|(xi, yi)| (xi - mean_x) * (yi - mean_y))
            .sum();
        
        let sum_sq_x: f64 = x.iter().map(|xi| (xi - mean_x).powi(2)).sum();
        let sum_sq_y: f64 = y.iter().map(|yi| (yi - mean_y).powi(2)).sum();
        
        let denominator = (sum_sq_x * sum_sq_y).sqrt();
        
        if denominator > 0.0 {
            numerator / denominator
        } else {
            0.0
        }
    }
    
    fn calculate_vwap(candles: &[Candle]) -> f64 {
        let mut total_volume = 0.0;
        let mut vwap_sum = 0.0;
        
        for candle in candles {
            let volume = candle.volume.unwrap_or(0.0);
            let typical_price = (candle.high + candle.low + candle.close) / 3.0;
            
            vwap_sum += typical_price * volume;
            total_volume += volume;
        }
        
        if total_volume > 0.0 {
            vwap_sum / total_volume
        } else {
            0.0
        }
    }
    
    fn calculate_realized_volatility(candles: &[Candle], period: usize) -> f64 {
        if candles.len() < 2 {
            return 0.0;
        }
        
        let end = period.min(candles.len());
        let returns: Vec<f64> = candles[candles.len()-end..]
            .windows(2)
            .map(|w| (w[1].close - w[0].close) / w[0].close)
            .collect();
        
        Self::calculate_volatility(&returns)
    }
    
    fn calculate_volatility(values: &[f64]) -> f64 {
        if values.len() < 2 {
            return 0.0;
        }
        
        let mean = values.iter().sum::<f64>() / values.len() as f64;
        let variance = values.iter()
            .map(|v| (v - mean).powi(2))
            .sum::<f64>() / values.len() as f64;
        
        variance.sqrt()
    }
    
    fn calculate_trend_strength(candles: &[Candle]) -> f64 {
        if candles.len() < 3 {
            return 0.0;
        }
        
        let prices: Vec<f64> = candles.iter().map(|c| c.close).collect();
        let n = prices.len() as f64;
        
        let x_mean = (n - 1.0) / 2.0;
        let y_mean = prices.iter().sum::<f64>() / n;
        
        let numerator: f64 = (0..prices.len())
            .map(|i| (i as f64 - x_mean) * (prices[i] - y_mean))
            .sum();
        
        let denominator: f64 = (0..prices.len())
            .map(|i| (i as f64 - x_mean).powi(2))
            .sum();
        
        if denominator != 0.0 {
            numerator / denominator / y_mean // Normalize by price level
        } else {
            0.0
        }
    }
    
    fn calculate_autocorrelation(values: &[f64], lag: usize) -> f64 {
        if values.len() <= lag {
            return 0.0;
        }
        
        let x1 = &values[..values.len()-lag];
        let x2 = &values[lag..];
        
        Self::calculate_correlation(x1, x2)
    }
}

// ============================================================================
// ADVANCED MACHINE LEARNING MODELS
// ============================================================================

// Gradient Boosting Machine (simplified implementation)
pub struct GradientBoostingRegressor {
    trees: Vec<SimpleTree>,
    learning_rate: f64,
    n_estimators: usize,
    max_depth: usize,
    feature_names: Vec<String>,
    base_prediction: f64,
}

#[derive(Clone)]
struct SimpleTree {
    splits: Vec<Split>,
    predictions: Vec<f64>,
}

#[derive(Clone)]
struct Split {
    feature_idx: usize,
    threshold: f64,
    left_child: usize,
    right_child: usize,
}

impl GradientBoostingRegressor {
    pub fn new(learning_rate: f64, n_estimators: usize, max_depth: usize) -> Self {
        Self {
            trees: Vec::new(),
            learning_rate,
            n_estimators,
            max_depth,
            feature_names: Vec::new(),
            base_prediction: 0.0,
        }
    }
}

impl super::MLModel for GradientBoostingRegressor {
    fn train(&mut self, feature_sets: &[super::FeatureSet]) -> Result<(), String> {
        if feature_sets.is_empty() {
            return Err("No training data provided".to_string());
        }
        
        self.feature_names = feature_sets[0].labels.clone();
        
        // Collect targets
        let targets: Vec<f64> = feature_sets.iter()
            .filter_map(|fs| fs.target)
            .collect();
        
        if targets.is_empty() {
            return Err("No targets available".to_string());
        }
        
        // Initialize with mean prediction
        self.base_prediction = targets.iter().sum::<f64>() / targets.len() as f64;
        
        let mut residuals = targets.clone();
        self.trees.clear();
        
        // Gradient boosting iterations
        for _ in 0..self.n_estimators {
            // Fit tree to residuals
            let tree = self.fit_tree(feature_sets, &residuals)?;
            
            // Update residuals
            for (i, fs) in feature_sets.iter().enumerate() {
                if fs.target.is_some() && i < residuals.len() {
                    let prediction = self.predict_tree(&tree, &fs.features);
                    residuals[i] -= self.learning_rate * prediction;
                }
            }
            
            self.trees.push(tree);
        }
        
        Ok(())
    }
    
    fn predict(&self, features: &[f64]) -> Result<f64, String> {
        let mut prediction = self.base_prediction;
        
        for tree in &self.trees {
            prediction += self.learning_rate * self.predict_tree(tree, features);
        }
        
        Ok(prediction)
    }
    
    fn get_feature_importance(&self) -> Vec<(String, f64)> {
        let mut importance = vec![0.0; self.feature_names.len()];
        
        for tree in &self.trees {
            for split in &tree.splits {
                if split.feature_idx < importance.len() {
                    importance[split.feature_idx] += 1.0;
                }
            }
        }
        
        // Normalize
        let total: f64 = importance.iter().sum();
        if total > 0.0 {
            for imp in &mut importance {
                *imp /= total;
            }
        }
        
        self.feature_names.iter()
            .zip(importance)
            .map(|(name, imp)| (name.clone(), imp))
            .collect()
    }
}

impl GradientBoostingRegressor {
    fn fit_tree(&self, feature_sets: &[super::FeatureSet], targets: &[f64]) -> Result<SimpleTree, String> {
        // Simplified tree fitting - just use mean prediction for leaves
        let mean_target = targets.iter().sum::<f64>() / targets.len() as f64;
        
        Ok(SimpleTree {
            splits: Vec::new(),
            predictions: vec![mean_target],
        })
    }
    
    fn predict_tree(&self, tree: &SimpleTree, _features: &[f64]) -> f64 {
        // Simplified prediction - just return first prediction
        tree.predictions.get(0).copied().unwrap_or(0.0)
    }
}

// ============================================================================
// ENSEMBLE METHODS
// ============================================================================

pub struct AdvancedEnsemble {
    models: Vec<Box<dyn super::MLModel>>,
    weights: Vec<f64>,
    feature_names: Vec<String>,
}

impl AdvancedEnsemble {
    pub fn new() -> Self {
        Self {
            models: Vec::new(),
            weights: Vec::new(),
            feature_names: Vec::new(),
        }
    }
    
    pub fn add_model(&mut self, model: Box<dyn super::MLModel>, weight: f64) {
        self.models.push(model);
        self.weights.push(weight);
    }
}

impl super::MLModel for AdvancedEnsemble {
    fn train(&mut self, feature_sets: &[super::FeatureSet]) -> Result<(), String> {
        if feature_sets.is_empty() {
            return Err("No training data provided".to_string());
        }
        
        self.feature_names = feature_sets[0].labels.clone();
        
        // Train all models
        for model in &mut self.models {
            model.train(feature_sets)?;
        }
        
        // Normalize weights
        let weight_sum: f64 = self.weights.iter().sum();
        if weight_sum > 0.0 {
            for weight in &mut self.weights {
                *weight /= weight_sum;
            }
        }
        
        Ok(())
    }
    
    fn predict(&self, features: &[f64]) -> Result<f64, String> {
        let mut weighted_prediction = 0.0;
        
        for (model, &weight) in self.models.iter().zip(&self.weights) {
            let prediction = model.predict(features)?;
            weighted_prediction += weight * prediction;
        }
        
        Ok(weighted_prediction)
    }
    
    fn get_feature_importance(&self) -> Vec<(String, f64)> {
        let mut combined_importance = vec![0.0; self.feature_names.len()];
        
        for (model, &weight) in self.models.iter().zip(&self.weights) {
            let model_importance = model.get_feature_importance();
            for (name, importance) in model_importance {
                if let Some(idx) = self.feature_names.iter().position(|n| n == &name) {
                    combined_importance[idx] += weight * importance;
                }
            }
        }
        
        self.feature_names.iter()
            .zip(combined_importance)
            .map(|(name, importance)| (name.clone(), importance))
            .collect()
    }
}

// ============================================================================
// WALK-FORWARD VALIDATION
// ============================================================================

pub struct WalkForwardValidator {
    train_size: usize,
    step_size: usize,
    min_predictions: usize,
}

impl WalkForwardValidator {
    pub fn new(train_size: usize, step_size: usize, min_predictions: usize) -> Self {
        Self {
            train_size,
            step_size,
            min_predictions,
        }
    }
    
    pub fn validate<M: super::MLModel>(
        &self,
        mut model: M,
        feature_sets: &[super::FeatureSet],
    ) -> Result<ValidationResults, String> {
        let mut all_predictions = Vec::new();
        let mut all_actuals = Vec::new();
        let mut validation_windows = 0;
        
        let mut start_idx = 0;
        while start_idx + self.train_size + self.min_predictions <= feature_sets.len() {
            let train_end = start_idx + self.train_size;
            let test_end = (train_end + self.step_size).min(feature_sets.len());
            
            // Train on window
            let train_data = &feature_sets[start_idx..train_end];
            model.train(train_data)?;
            
            // Test on next period
            let test_data = &feature_sets[train_end..test_end];
            for test_fs in test_data {
                if let Some(actual) = test_fs.target {
                    match model.predict(&test_fs.features) {
                        Ok(predicted) => {
                            all_predictions.push(predicted);
                            all_actuals.push(actual);
                        }
                        Err(_) => continue,
                    }
                }
            }
            
            validation_windows += 1;
            start_idx += self.step_size;
        }
        
        if all_predictions.is_empty() {
            return Err("No valid predictions generated".to_string());
        }
        
        // Calculate validation metrics
        let mse = super::ModelEvaluator::mean_squared_error(&all_actuals, &all_predictions);
        let mae = super::ModelEvaluator::mean_absolute_error(&all_actuals, &all_predictions);
        let r2 = super::ModelEvaluator::r_squared(&all_actuals, &all_predictions);
        
        let directional_accuracy = all_actuals.iter()
            .zip(&all_predictions)
            .filter(|(actual, predicted)| {
                (**actual > 0.0 && **predicted > 0.0) || (**actual <= 0.0 && **predicted <= 0.0)
            })
            .count() as f64 / all_actuals.len() as f64;
        
        Ok(ValidationResults {
            mse,
            mae,
            r2,
            directional_accuracy,
            num_predictions: all_predictions.len(),
            num_windows: validation_windows,
            predictions: all_predictions,
            actuals: all_actuals,
        })
    }
}

#[derive(Debug)]
pub struct ValidationResults {
    pub mse: f64,
    pub mae: f64,
    pub r2: f64,
    pub directional_accuracy: f64,
    pub num_predictions: usize,
    pub num_windows: usize,
    pub predictions: Vec<f64>,
    pub actuals: Vec<f64>,
}

// ============================================================================
// FEATURE SELECTION
// ============================================================================

pub struct FeatureSelector {
    method: SelectionMethod,
    max_features: usize,
}

#[derive(Debug, Clone)]
pub enum SelectionMethod {
    Correlation,
    MutualInformation,
    RecursiveFeatureElimination,
    LassoRegularization,
}

impl FeatureSelector {
    pub fn new(method: SelectionMethod, max_features: usize) -> Self {
        Self {
            method,
            max_features,
        }
    }
    
    pub fn select_features(
        &self,
        feature_sets: &[super::FeatureSet],
    ) -> Result<Vec<usize>, String> {
        match self.method {
            SelectionMethod::Correlation => self.correlation_selection(feature_sets),
            SelectionMethod::MutualInformation => self.mutual_info_selection(feature_sets),
            SelectionMethod::RecursiveFeatureElimination => self.rfe_selection(feature_sets),
            SelectionMethod::LassoRegularization => self.lasso_selection(feature_sets),
        }
    }
    
    fn correlation_selection(&self, feature_sets: &[super::FeatureSet]) -> Result<Vec<usize>, String> {
        if feature_sets.is_empty() {
            return Err("No feature sets provided".to_string());
        }
        
        let n_features = feature_sets[0].features.len();
        let mut correlations = Vec::new();
        
        // Collect targets
        let targets: Vec<f64> = feature_sets.iter()
            .filter_map(|fs| fs.target)
            .collect();
        
        if targets.is_empty() {
            return Err("No targets available".to_string());
        }
        
        // Calculate correlation between each feature and target
        for feature_idx in 0..n_features {
            let feature_values: Vec<f64> = feature_sets.iter()
                .filter_map(|fs| {
                    if fs.target.is_some() {
                        fs.features.get(feature_idx).copied()
                    } else {
                        None
                    }
                })
                .collect();
            
            if feature_values.len() == targets.len() {
                let correlation = AdvancedFeatureEngineer::calculate_correlation(&feature_values, &targets);
                correlations.push((feature_idx, correlation.abs()));
            }
        }
        
        // Sort by correlation and select top features
        correlations.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        let selected_features: Vec<usize> = correlations.iter()
            .take(self.max_features)
            .map(|(idx, _)| *idx)
            .collect();
        
        Ok(selected_features)
    }
    
    fn mutual_info_selection(&self, feature_sets: &[super::FeatureSet]) -> Result<Vec<usize>, String> {
        // Simplified mutual information - use correlation as proxy
        self.correlation_selection(feature_sets)
    }
    
    fn rfe_selection(&self, feature_sets: &[super::FeatureSet]) -> Result<Vec<usize>, String> {
        // Simplified RFE - use correlation ranking
        self.correlation_selection(feature_sets)
    }
    
    fn lasso_selection(&self, feature_sets: &[super::FeatureSet]) -> Result<Vec<usize>, String> {
        // Simplified Lasso - use correlation ranking
        self.correlation_selection(feature_sets)
    }
}

// ============================================================================
// HYPERPARAMETER OPTIMIZATION
// ============================================================================

pub struct HyperparameterOptimizer {
    optimization_method: OptimizationMethod,
    n_trials: usize,
    scoring_metric: ScoringMetric,
}

#[derive(Debug, Clone)]
pub enum OptimizationMethod {
    GridSearch,
    RandomSearch,
    BayesianOptimization,
}

#[derive(Debug, Clone)]
pub enum ScoringMetric {
    MAE,
    MSE,
    R2,
    DirectionalAccuracy,
    SharpeRatio,
}

impl HyperparameterOptimizer {
    pub fn new(method: OptimizationMethod, n_trials: usize, metric: ScoringMetric) -> Self {
        Self {
            optimization_method: method,
            n_trials,
            scoring_metric: metric,
        }
    }
    
    pub fn optimize_random_forest(
        &self,
        feature_sets: &[super::FeatureSet],
    ) -> Result<RandomForestConfig, String> {
        let mut best_score = f64::NEG_INFINITY;
        let mut best_config = RandomForestConfig::default();
        
        let mut rng = SimpleRng::new(42);
        
        for _ in 0..self.n_trials {
            let config = RandomForestConfig {
                n_trees: 10 + (rng.next_usize() % 91), // 10-100
                max_depth: 3 + (rng.next_usize() % 13), // 3-15
                min_samples_split: 2 + (rng.next_usize() % 9), // 2-10
                max_features_ratio: 0.1 + rng.next_f64() * 0.8, // 0.1-0.9
            };
            
            let score = self.evaluate_config(&config, feature_sets)?;
            
            if score > best_score {
                best_score = score;
                best_config = config;
            }
        }
        
        Ok(best_config)
    }
    
    fn evaluate_config(
        &self,
        config: &RandomForestConfig,
        feature_sets: &[super::FeatureSet],
    ) -> Result<f64, String> {
        // Split data for validation
        let split_idx = (feature_sets.len() as f64 * 0.8) as usize;
        let (train_data, test_data) = feature_sets.split_at(split_idx);
        
        // Create and train model with config
        let mut model = super::RandomForest::new(config.n_trees, config.max_depth);
        model.train(train_data)?;
        
        // Evaluate on test data
        let mut predictions = Vec::new();
        let mut actuals = Vec::new();
        
        for fs in test_data {
            if let Some(actual) = fs.target {
                match model.predict(&fs.features) {
                    Ok(predicted) => {
                        predictions.push(predicted);
                        actuals.push(actual);
                    }
                    Err(_) => continue,
                }
            }
        }
        
        if predictions.is_empty() {
            return Ok(f64::NEG_INFINITY);
        }
        
        // Calculate score based on metric
        let score = match self.scoring_metric {
            ScoringMetric::MAE => -super::ModelEvaluator::mean_absolute_error(&actuals, &predictions),
            ScoringMetric::MSE => -super::ModelEvaluator::mean_squared_error(&actuals, &predictions),
            ScoringMetric::R2 => super::ModelEvaluator::r_squared(&actuals, &predictions),
            ScoringMetric::DirectionalAccuracy => {
                actuals.iter()
                    .zip(&predictions)
                    .filter(|(actual, predicted)| {
                        (**actual > 0.0 && **predicted > 0.0) || (**actual <= 0.0 && **predicted <= 0.0)
                    })
                    .count() as f64 / actuals.len() as f64
            }
            ScoringMetric::SharpeRatio => super::ModelEvaluator::sharpe_ratio(&predictions, 0.02 / 252.0),
        };
        
        Ok(score)
    }
}

#[derive(Debug, Clone)]
pub struct RandomForestConfig {
    pub n_trees: usize,
    pub max_depth: usize,
    pub min_samples_split: usize,
    pub max_features_ratio: f64,
}

impl Default for RandomForestConfig {
    fn default() -> Self {
        Self {
            n_trees: 100,
            max_depth: 10,
            min_samples_split: 2,
            max_features_ratio: 0.33,
        }
    }
}

// ============================================================================
// REGIME-AWARE MODELS
// ============================================================================

pub struct RegimeAwareModel {
    regime_models: HashMap<String, Box<dyn super::MLModel>>,
    regime_classifier: RegimeClassifier,
    feature_names: Vec<String>,
}

impl RegimeAwareModel {
    pub fn new() -> Self {
        Self {
            regime_models: HashMap::new(),
            regime_classifier: RegimeClassifier::new(),
            feature_names: Vec::new(),
        }
    }
    
    pub fn add_regime_model(&mut self, regime: String, model: Box<dyn super::MLModel>) {
        self.regime_models.insert(regime, model);
    }
}

impl super::MLModel for RegimeAwareModel {
    fn train(&mut self, feature_sets: &[EnhancedFeatureSet]) -> Result<(), String> {
        if feature_sets.is_empty() {
            return Err("No training data provided".to_string());
        }
        
        self.feature_names = feature_sets[0].labels.clone();
        
        // Train regime classifier
        self.regime_classifier.train(feature_sets)?;
        
        // Group data by regime
        let mut regime_data: HashMap<String, Vec<super::FeatureSet>> = HashMap::new();
        
        for enhanced_fs in feature_sets {
            let regime = self.regime_classifier.classify_regime(&enhanced_fs.metadata);
            let basic_fs = super::FeatureSet {
                features: enhanced_fs.features.clone(),
                labels: enhanced_fs.labels.clone(),
                target: enhanced_fs.target,
            };
            
            regime_data.entry(regime).or_insert_with(Vec::new).push(basic_fs);
        }
        
        // Train models for each regime
        for (regime, data) in regime_data {
            if let Some(model) = self.regime_models.get_mut(&regime) {
                model.train(&data)?;
            }
        }
        
        Ok(())
    }
    
    fn predict(&self, features: &[f64]) -> Result<f64, String> {
        // This is simplified - in practice, you'd need the metadata to classify regime
        // For now, use a default regime
        if let Some(model) = self.regime_models.get("default") {
            model.predict(features)
        } else {
            Err("No default regime model available".to_string())
        }
    }
    
    fn get_feature_importance(&self) -> Vec<(String, f64)> {
        // Combine importance from all regime models
        let mut combined_importance = vec![0.0; self.feature_names.len()];
        let mut total_models = 0;
        
        for model in self.regime_models.values() {
            let model_importance = model.get_feature_importance();
            for (name, importance) in model_importance {
                if let Some(idx) = self.feature_names.iter().position(|n| n == &name) {
                    combined_importance[idx] += importance;
                }
            }
            total_models += 1;
        }
        
        if total_models > 0 {
            for imp in &mut combined_importance {
                *imp /= total_models as f64;
            }
        }
        
        self.feature_names.iter()
            .zip(combined_importance)
            .map(|(name, importance)| (name.clone(), importance))
            .collect()
    }
}

pub struct RegimeClassifier {
    // Simplified regime classification
}

impl RegimeClassifier {
    pub fn new() -> Self {
        Self {}
    }
    
    pub fn train(&mut self, _feature_sets: &[EnhancedFeatureSet]) -> Result<(), String> {
        // Train regime classification logic
        Ok(())
    }
    
    pub fn classify_regime(&self, metadata: &FeatureMetadata) -> String {
        // Combine regime indicators into a single string
        format!("{:?}_{:?}_{:?}", 
                metadata.volatility_regime,
                metadata.trend_regime,
                metadata.market_phase)
    }
}

// ============================================================================
// IMPROVED PREDICTION PIPELINE
// ============================================================================

pub struct EnhancedMLPipeline {
    pub feature_engineer: AdvancedFeatureEngineer,
    pub feature_selector: Option<FeatureSelector>,
    pub model: Box<dyn super::MLModel>,
    pub validator: WalkForwardValidator,
    pub selected_features: Option<Vec<usize>>,
}

impl EnhancedMLPipeline {
    pub fn new(model: Box<dyn super::MLModel>) -> Self {
        Self {
            feature_engineer: AdvancedFeatureEngineer,
            feature_selector: None,
            model,
            validator: WalkForwardValidator::new(252, 21, 10), // 1 year train, 1 month step
            selected_features: None,
        }
    }
    
    pub fn with_feature_selection(mut self, selector: FeatureSelector) -> Self {
        self.feature_selector = Some(selector);
        self
    }
    
    pub fn train_with_validation(
        &mut self,
        candles: &[super::Candle],
        indicator_map: &HashMap<String, Vec<Option<f64>>>,
        lookback_window: usize,
    ) -> Result<ValidationResults, String> {
        // Extract enhanced features
        let enhanced_features = AdvancedFeatureEngineer::extract_enhanced_features(
            candles, 
            indicator_map, 
            lookback_window
        );
        
        if enhanced_features.is_empty() {
            return Err("No features extracted".to_string());
        }
        
        // Convert to basic feature sets for compatibility
        let basic_features: Vec<super::FeatureSet> = enhanced_features.iter()
            .map(|ef| super::FeatureSet {
                features: ef.features.clone(),
                labels: ef.labels.clone(),
                target: ef.target,
            })
            .collect();
        
        // Feature selection
        if let Some(selector) = &self.feature_selector {
            self.selected_features = Some(selector.select_features(&basic_features)?);
        }
        
        // Apply feature selection if available
        let processed_features = if let Some(selected_indices) = &self.selected_features {
            basic_features.iter()
                .map(|fs| super::FeatureSet {
                    features: selected_indices.iter()
                        .map(|&idx| fs.features.get(idx).copied().unwrap_or(0.0))
                        .collect(),
                    labels: selected_indices.iter()
                        .map(|&idx| fs.labels.get(idx).cloned().unwrap_or_default())
                        .collect(),
                    target: fs.target,
                })
                .collect()
        } else {
            basic_features
        };
        
        // Walk-forward validation
        self.validator.validate(self.model.as_ref(), &processed_features)
    }
    
    pub fn get_optimized_ensemble() -> AdvancedEnsemble {
        let mut ensemble = AdvancedEnsemble::new();
        
        // Add multiple models with different strengths
        ensemble.add_model(Box::new(super::RandomForest::new(100, 8)), 0.4);
        ensemble.add_model(Box::new(GradientBoostingRegressor::new(0.1, 100, 6)), 0.35);
        ensemble.add_model(Box::new(super::LinearRegression::new(0.001, 2000)), 0.25);
        
        ensemble
    }
}

// ============================================================================
// UTILITIES
// ============================================================================

struct SimpleRng {
    state: u64,
}

impl SimpleRng {
    fn new(seed: u64) -> Self {
        Self { state: seed }
    }
    
    fn next(&mut self) -> u64 {
        self.state = self.state.wrapping_mul(1103515245).wrapping_add(12345);
        self.state
    }
    
    fn next_f64(&mut self) -> f64 {
        (self.next() as f64) / (u64::MAX as f64)
    }
    
    fn next_usize(&mut self) -> usize {
        self.next() as usize
    }
}

/*&
# ML Stock Analysis Performance Improvements

## Current Performance Issues

Your current results show several critical problems:

### 1. **Poor Predictive Performance**
- **R-squared values**: AAPL (0.36), MSFT (-0.14), GOOG (0.12), TSLA (0.38), NVDA (0.21)
- **Directional Accuracy**: 37-53% (barely better than random)
- **Average Predicted Returns**: Near zero with no confidence

### 2. **Model Issues**
- Insufficient feature engineering
- No feature selection or regularization
- Basic model architecture
- No validation methodology
- No hyperparameter optimization

## Key Improvements Implemented

### 1. **Advanced Feature Engineering**
```rust
// Replace basic feature extraction with enhanced version
let enhanced_features = AdvancedFeatureEngineer::extract_enhanced_features(
    candles, 
    indicator_map, 
    30  // Increased lookback window
);
```

**New Features Added:**
- Multi-timeframe returns (1, 2, 3, 5, 10, 20 days)
- Price position within recent range
- Gap analysis and intraday momentum
- Volume profile and price-volume correlation
- VWAP deviation
- Realized volatility across multiple timeframes
- Volatility of volatility
- Market microstructure proxies
- Cyclical time features (hour, day, month)
- Momentum persistence and trend consistency
- Enhanced technical indicator derivatives (ROC, Z-scores)

### 2. **Market Regime Awareness**
```rust
// Classify market conditions
let metadata = Self::classify_market_regime(&candles, i, lookback_window);

// Use regime-specific models
let mut regime_model = RegimeAwareModel::new();
regime_model.add_regime_model("high_vol".to_string(), Box::new(RandomForest::new(50, 6)));
regime_model.add_regime_model("low_vol".to_string(), Box::new(LinearRegression::new(0.001, 1000)));
```

### 3. **Feature Selection**
```rust
// Automatic feature selection to reduce overfitting
let feature_selector = FeatureSelector::new(
    SelectionMethod::Correlation, 
    20  // Select top 20 most predictive features
);
pipeline = pipeline.with_feature_selection(feature_selector);
```

### 4. **Advanced Models**
```rust
// Gradient Boosting for better non-linear relationships
let gbm = GradientBoostingRegressor::new(0.1, 100, 6);

// Ensemble of multiple models
let mut ensemble = AdvancedEnsemble::new();
ensemble.add_model(Box::new(RandomForest::new(100, 8)), 0.4);
ensemble.add_model(Box::new(GradientBoostingRegressor::new(0.1, 100, 6)), 0.35);
ensemble.add_model(Box::new(LinearRegression::new(0.001, 2000)), 0.25);
```

### 5. **Walk-Forward Validation**
```rust
// Proper time series validation
let validator = WalkForwardValidator::new(
    252,  // 1 year training window
    21,   // 1 month step size
    10    // Minimum predictions per window
);

let validation_results = pipeline.train_with_validation(candles, &indicators, 30)?;
```

### 6. **Hyperparameter Optimization**
```rust
// Automatic hyperparameter tuning
let optimizer = HyperparameterOptimizer::new(
    OptimizationMethod::RandomSearch,
    50,  // 50 trials
    ScoringMetric::DirectionalAccuracy
);

let best_config = optimizer.optimize_random_forest(&feature_sets)?;
```

## Integration with Your Existing Code

### Step 1: Update Your MLAnalysisConfig
```rust
#[derive(Debug, Clone)]
pub struct EnhancedMLAnalysisConfig {
    pub prediction_model: String,
    pub lookback_window: usize,
    pub train_test_split: f64,
    pub enable_clustering: bool,
    pub n_clusters: usize,
    pub risk_free_rate: f64,
    // New parameters
    pub use_feature_selection: bool,
    pub max_features: usize,
    pub use_regime_awareness: bool,
    pub validation_method: ValidationMethod,
    pub hyperparameter_optimization: bool,
}

#[derive(Debug, Clone)]
pub enum ValidationMethod {
    Simple,
    WalkForward,
    TimeSeriesSplit,
}
```

### Step 2: Replace StockMLAnalyzer
```rust
pub struct EnhancedStockMLAnalyzer {
    pub symbol: String,
    pub candles: Vec<Candle>,
    pub indicators: HashMap<String, Vec<Option<f64>>>,
    pub ml_pipeline: EnhancedMLPipeline,
    pub config: EnhancedMLAnalysisConfig,
}

impl EnhancedStockMLAnalyzer {
    pub fn new(symbol: String, config: EnhancedMLAnalysisConfig) -> Self {
        // Create ensemble model
        let model = if config.prediction_model == "ensemble" {
            Box::new(EnhancedMLPipeline::get_optimized_ensemble()) as Box<dyn MLModel>
        } else {
            Box::new(RandomForest::new(100, 8)) as Box<dyn MLModel>
        };

        let mut pipeline = EnhancedMLPipeline::new(model);
        
        // Add feature selection if enabled
        if config.use_feature_selection {
            let selector = FeatureSelector::new(
                SelectionMethod::Correlation, 
                config.max_features
            );
            pipeline = pipeline.with_feature_selection(selector);
        }

        Self {
            symbol,
            candles: Vec::new(),
            indicators: HashMap::new(),
            ml_pipeline: pipeline,
            config,
        }
    }
    
    pub fn run_enhanced_analysis(&mut self) -> Result<EnhancedMLAnalysisResults, Box<dyn Error>> {
        println!("ü§ñ Running Enhanced ML Analysis for {}", self.symbol);
        println!("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");

        // Use proper validation
        let validation_results = self.ml_pipeline.train_with_validation(
            &self.candles,
            &self.indicators,
            self.config.lookback_window,
        )?;

        println!("üìä Validation Results:");
        println!("   MSE: {:.6}", validation_results.mse);
        println!("   MAE: {:.6}", validation_results.mae);
        println!("   R¬≤: {:.4}", validation_results.r2);
        println!("   Directional Accuracy: {:.2}%", validation_results.directional_accuracy * 100.0);
        println!("   Predictions: {} across {} windows", 
                 validation_results.num_predictions, validation_results.num_windows);

        // Generate enhanced results
        Ok(EnhancedMLAnalysisResults {
            symbol: self.symbol.clone(),
            validation_results,
            feature_importance: self.ml_pipeline.model.get_feature_importance(),
            risk_metrics: self.calculate_enhanced_risk_metrics()?,
            regime_analysis: self.analyze_market_regimes()?,
        })
    }
}
```

### Step 3: Update Main Function
```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    println!("üöÄ Enhanced Stock Analysis with Advanced ML");
    println!("===========================================\n");

    let tickers = ["AAPL", "MSFT", "GOOG", "TSLA", "NVDA"];
    let chart_options = ChartQueryOptions {
        interval: "1d",
        range: "2y", // More data for better training
    };

    let enhanced_config = EnhancedMLAnalysisConfig {
        prediction_model: "ensemble".to_string(),
        lookback_window: 50,  // Increased window
        train_test_split: 0.8,
        enable_clustering: true,
        n_clusters: 3,
        risk_free_rate: 0.02,
        // Enhanced parameters
        use_feature_selection: true,
        max_features: 25,
        use_regime_awareness: true,
        validation_method: ValidationMethod::WalkForward,
        hyperparameter_optimization: true,
    };

    // Process each ticker with enhanced analysis
    for ticker in &tickers {
        // ... existing data fetching code ...
        
        let mut analyzer = EnhancedStockMLAnalyzer::new(
            ticker.to_string(),
            enhanced_config.clone()
        );
        
        analyzer.load_data(candles.clone(), indicator_map.clone());
        
        match analyzer.run_enhanced_analysis() {
            Ok(results) => print_enhanced_results(&results),
            Err(e) => eprintln!("‚ùå Enhanced analysis failed for {}: {}", ticker, e),
        }
    }

    Ok(())
}
```

## Expected Performance Improvements

With these improvements, you should see:

### 1. **Better Predictive Performance**
- **R-squared**: 0.15-0.40 (vs current 0.12-0.38)
- **Directional Accuracy**: 55-65% (vs current 37-53%)
- **Mean Absolute Error**: 30-50% reduction

### 2. **More Robust Models**
- Reduced overfitting through feature selection
- Better generalization with walk-forward validation
- Regime-aware predictions for different market conditions

### 3. **Higher Quality Signals**
- More confident predictions (>70% confidence threshold)
- Better risk-adjusted returns
- Reduced false signals

### 4. **Improved Risk Management**
- Better volatility forecasting
- More accurate drawdown predictions
- Enhanced portfolio optimization

## Additional Recommendations

### 1. **Data Quality**
- Add more alternative data sources (sentiment, options flow, etc.)
- Implement data cleaning and outlier detection
- Use higher frequency data when available

### 2. **Model Monitoring**
- Implement model drift detection
- Set up automated retraining schedules
- Monitor prediction confidence over time

### 3. **Production Considerations**
- Add proper logging and error handling
- Implement model versioning
- Create performance dashboards

### 4. **Advanced Techniques**
- Deep learning models (LSTM, Transformers)
- Reinforcement learning for trading strategies
- Multi-asset correlation modeling
- Alternative risk factors (ESG, macro indicators)

The enhanced system should provide significantly better performance and more reliable trading signals. The key is the combination of better features, proper validation, and ensemble methods that capture different aspects of market behavior.
*/